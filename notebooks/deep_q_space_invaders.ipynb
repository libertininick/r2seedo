{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning: Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from tempfile import TemporaryFile\n",
    "from typing import Any, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import dotenv_values\n",
    "from tqdm import tqdm\n",
    "\n",
    "from r2seedo.io import load_keypair, load_n_verify_model, sign_n_save_model\n",
    "from r2seedo.models.dqn import (\n",
    "    DQN,\n",
    "    DQNConfig,\n",
    "    DQNTrainingConfig,\n",
    "    calculate_loss,\n",
    "    update_target_network,\n",
    ")\n",
    "from r2seedo.utils import get_device\n",
    "from r2seedo.utils.environment import (\n",
    "    AtariEnvConfig,\n",
    "    capture_replay,\n",
    "    copy_n_adj_next_observation,\n",
    "    get_replay_buffer,\n",
    ")\n",
    "from r2seedo.utils.training import (\n",
    "    ExplorationConfig,\n",
    "    LearningRateConfig,\n",
    "    ReplayBufferConfig,\n",
    "    ScheduleMode,\n",
    "    TrainingConfig,\n",
    "    set_learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"env_id\": \"SpaceInvadersNoFrameskip-v4\",\n",
      "  \"clip_reward\": true,\n",
      "  \"frame_skip\": 4,\n",
      "  \"frame_stack\": 4,\n",
      "  \"grayscale\": true,\n",
      "  \"noop_max\": 30,\n",
      "  \"screen_size\": [\n",
      "    84,\n",
      "    84\n",
      "  ],\n",
      "  \"terminal_on_life_loss\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Environment configuration\n",
    "env_id = \"SpaceInvadersNoFrameskip-v4\"\n",
    "env_config = AtariEnvConfig(env_id=env_id)\n",
    "eval_env_config = AtariEnvConfig(**{**env_config.to_dict(), \"clip_reward\": False})\n",
    "print(env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hyperparams(trial: optuna.Trial) -> dict[str, Any]:\n",
    "    \"\"\"Sample hyperparameters for training a DQN model.\"\"\"\n",
    "    dqn_params = {\n",
    "        # \"exp_hidden_dim\": trial.suggest_int(\"exp_hidden_dim\", 3, 7),  # [8, 128]\n",
    "    }\n",
    "    train_params = {\n",
    "        # \"lr_start\": trial.suggest_float(\"lr_start\", 2e-5, 1e-2, log=True),\n",
    "        # \"lr_constant_fraction\": trial.suggest_float(\"lr_constant_fraction\", 0.0, 0.5),\n",
    "        \"exploration_fraction\": trial.suggest_float(\"exploration_fraction\", 0.05, 0.7),\n",
    "        # \"explore_anneal_mode\": trial.suggest_categorical(\n",
    "        #     \"explore_anneal_mode\", [ScheduleMode.LINEAR, ScheduleMode.EXPONENTIAL]\n",
    "        # ),\n",
    "        \"gamma\": 1.0 - trial.suggest_float(\"gamma\", 0.001, 0.2),\n",
    "        # \"double_q\": trial.suggest_categorical(\"double_q\", [True, False]),\n",
    "        \"target_inertia\": 1.0\n",
    "        - trial.suggest_float(\"target_inertia\", 0.0001, 0.1, log=True),\n",
    "    }\n",
    "\n",
    "    return {\"dqn_params\": dqn_params, \"train_params\": train_params}\n",
    "\n",
    "\n",
    "def get_configs_from_hyperparams(\n",
    "    total_timesteps: int,\n",
    "    evaluation_rate: float,\n",
    "    observation_shape: tuple[int, int, int],\n",
    "    num_actions: int,\n",
    "    hyperparams: dict[str, Any] | None = None,\n",
    ") -> tuple[DQNConfig, TrainingConfig]:\n",
    "    \"\"\"Initialize configurations for DQN model & training from hyperparameters.\"\"\"\n",
    "    # Unpack hyperparameters\n",
    "    if hyperparams is None:\n",
    "        hyperparams = {}\n",
    "    dqn_params: dict[str, Any] = hyperparams.get(\"dqn_params\", {})\n",
    "    train_params: dict[str, Any] = hyperparams.get(\"train_params\", {})\n",
    "\n",
    "    # DQN configuration\n",
    "    dqn_config = DQNConfig(\n",
    "        observation_shape=observation_shape,\n",
    "        num_actions=num_actions,\n",
    "        hidden_dim=2 ** dqn_params.get(\"exp_hidden_dim\", 6),\n",
    "    )\n",
    "\n",
    "    # Training configuration\n",
    "    training_config = TrainingConfig(\n",
    "        total_timesteps=total_timesteps,\n",
    "        num_envs=3,\n",
    "        train_freq=4,\n",
    "        evaluation_rate=evaluation_rate,\n",
    "        learning_rate_config=LearningRateConfig(\n",
    "            lr_start=train_params.get(\"lr_start\", 1e-3),\n",
    "            constant_fraction=train_params.get(\"lr_constant_fraction\", 0.25),\n",
    "        ),\n",
    "        exploration_config=ExplorationConfig(\n",
    "            exploration_fraction=train_params.get(\"exploration_fraction\", 0.2),\n",
    "            anneal_mode=train_params.get(\"explore_anneal_mode\", ScheduleMode.LINEAR),\n",
    "        ),\n",
    "        replay_buffer_config=ReplayBufferConfig(buffer_fraction=0.25, batch_size=32),\n",
    "        other=DQNTrainingConfig(\n",
    "            gamma=train_params.get(\"gamma\", 0.99),\n",
    "            double_q=train_params.get(\"double_q\", True),\n",
    "            target_inertia=train_params.get(\"target_inertia\", 0.999),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return dqn_config, training_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationResults(NamedTuple):\n",
    "    \"\"\"Evaluation results.\"\"\"\n",
    "\n",
    "    mean_score: float\n",
    "    score_std: float\n",
    "    mean_len: float\n",
    "    len_std: float\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    dqn: DQN,\n",
    "    env_config: AtariEnvConfig,\n",
    "    epsilon: float = 0,\n",
    "    num_episodes: int = 30,\n",
    "    seed: int | None = 1234,\n",
    ") -> EvaluationResults:\n",
    "    \"\"\"Evaluate DQN agent on Atari environment.\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    dqn.eval()\n",
    "\n",
    "    # Initialize environment\n",
    "    evaluation_env = env_config.make_env(num_envs=1, seed=seed)\n",
    "\n",
    "    # Evaluate model\n",
    "    scores = []\n",
    "    episode_lens = []\n",
    "    for _ in range(num_episodes):\n",
    "        # Reset environment\n",
    "        obs = evaluation_env.reset()\n",
    "        rewards: list[float] = []\n",
    "        done = False\n",
    "\n",
    "        # Run episode\n",
    "        while not done:\n",
    "            # Choose action\n",
    "            with torch.no_grad():\n",
    "                action = (\n",
    "                    dqn.get_action(\n",
    "                        observation=torch.from_numpy(obs).to(dqn.device),\n",
    "                        epsilon=epsilon,\n",
    "                    )\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "            # Step environment\n",
    "            obs, reward, done, _ = evaluation_env.step(action)\n",
    "\n",
    "            # Update episode reward\n",
    "            rewards.append(reward.item())\n",
    "\n",
    "        # Tabulate episode results\n",
    "        scores.append(sum(rewards))\n",
    "        episode_lens.append(len(rewards))\n",
    "\n",
    "    # Close environment\n",
    "    evaluation_env.close()\n",
    "\n",
    "    # Return results\n",
    "    return EvaluationResults(\n",
    "        mean_score=np.mean(scores).item(),\n",
    "        score_std=np.std(scores).item(),\n",
    "        mean_len=np.mean(episode_lens).item(),\n",
    "        len_std=np.std(episode_lens).item(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(\n",
    "    env_config: AtariEnvConfig,\n",
    "    dqn_config: DQNConfig,\n",
    "    training_config: TrainingConfig,\n",
    "    eval_env_config: AtariEnvConfig | None = None,\n",
    "    trial: optuna.Trial | None = None,\n",
    "    device: torch.device | None = None,\n",
    ") -> tuple[DQN, pd.DataFrame]:\n",
    "    \"\"\"Initialize and train a DQN model.\"\"\"\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "\n",
    "    # Initialize training environment\n",
    "    training_env = env_config.make_env(num_envs=training_config.num_envs)\n",
    "\n",
    "    # Initialize networks\n",
    "    online_net = DQN(dqn_config).to(device)\n",
    "    target_net = DQN(dqn_config).to(device).eval()\n",
    "\n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = get_replay_buffer(\n",
    "        env=training_env,\n",
    "        buffer_size=training_config.replay_buffer_config.get_buffer_size(\n",
    "            training_config.total_timesteps\n",
    "        ),\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = training_config.optimizer_cls(online_net.parameters())\n",
    "\n",
    "    # Parameter schedules over training\n",
    "    param_schedule = training_config.get_parameter_schedule()\n",
    "\n",
    "    # Reset environment and get initial observation\n",
    "    obs = training_env.reset()\n",
    "\n",
    "    # Initialize evaluation statistics\n",
    "    if eval_env_config is None:\n",
    "        # Use training environment configuration for evaluation\n",
    "        eval_env_config = env_config\n",
    "    eval_stats = [evaluate(online_net, eval_env_config)]\n",
    "\n",
    "    # Create a temporary file to store model state\n",
    "    state_dict_fp = TemporaryFile()\n",
    "    torch.save(online_net.state_dict(), state_dict_fp)\n",
    "    state_dict_fp.seek(0)\n",
    "\n",
    "    # Run training loop\n",
    "    for step_i in tqdm(range(training_config.total_timesteps)):\n",
    "        # Set online network to training mode\n",
    "        online_net.train()\n",
    "\n",
    "        # Get action from online network\n",
    "        with torch.no_grad():\n",
    "            action = (\n",
    "                online_net.get_action(\n",
    "                    observation=torch.from_numpy(obs).to(device),\n",
    "                    epsilon=param_schedule[step_i].exploration_rate,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        # Take step in environment given action\n",
    "        next_obs, reward, termination, infos = training_env.step(action)\n",
    "\n",
    "        # Add to replay buffer\n",
    "        replay_buffer.add(\n",
    "            obs=obs,\n",
    "            next_obs=copy_n_adj_next_observation(next_obs, termination, infos),\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            done=termination,\n",
    "            infos=infos,\n",
    "        )\n",
    "\n",
    "        # Update observation\n",
    "        obs = next_obs\n",
    "\n",
    "        # Train online network\n",
    "        if (step_i % training_config.train_freq == 0) and (\n",
    "            step_i >= training_config.replay_buffer_config.batch_size\n",
    "        ):\n",
    "            # Sample batch from replay buffer\n",
    "            samples = replay_buffer.sample(\n",
    "                training_config.replay_buffer_config.batch_size\n",
    "            )\n",
    "\n",
    "            # Compute TD loss relative to target network's value estimate\n",
    "            loss = calculate_loss(\n",
    "                online_net,\n",
    "                target_net,\n",
    "                samples,\n",
    "                gamma=training_config.other.gamma,\n",
    "                double_q=training_config.other.double_q,\n",
    "            )\n",
    "\n",
    "            # Update online network\n",
    "            set_learning_rate(optimizer, param_schedule[step_i].learning_rate)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update target network\n",
    "            update_target_network(\n",
    "                target_net,\n",
    "                online_net,\n",
    "                target_inertia=training_config.other.target_inertia,\n",
    "            )\n",
    "\n",
    "        # Evaluate\n",
    "        if step_i % training_config.evaluation_freq == 0:\n",
    "            # Evaluate online network\n",
    "            stats_i = evaluate(online_net, eval_env_config)\n",
    "            eval_stats.append(stats_i)\n",
    "\n",
    "            # Save model state\n",
    "            if stats_i.mean_score >= max(s.mean_score for s in eval_stats):\n",
    "                torch.save(online_net.state_dict(), state_dict_fp)\n",
    "                state_dict_fp.seek(0)\n",
    "\n",
    "            if trial is not None:\n",
    "                # Report intermediate evaluation results to Optuna\n",
    "                trial.report(stats_i.mean_score, step_i)\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    # Close environment and prune trial\n",
    "                    training_env.close()\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Close environment\n",
    "    training_env.close()\n",
    "\n",
    "    # Evaluate final model\n",
    "    stats_i = evaluate(online_net, eval_env_config)\n",
    "    eval_stats.append(stats_i)\n",
    "\n",
    "    if stats_i.mean_score < max(s.mean_score for s in eval_stats):\n",
    "        # Load best model state\n",
    "        online_net.load_state_dict(torch.load(state_dict_fp))\n",
    "    state_dict_fp.close()\n",
    "\n",
    "    # Convert evaluation statistics to DataFrame\n",
    "    eval_stats = pd.DataFrame(eval_stats)\n",
    "\n",
    "    return online_net, eval_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial: optuna.Trial,\n",
    "    env_config: AtariEnvConfig,\n",
    "    eval_env_config: AtariEnvConfig | None,\n",
    "    observation_shape: tuple[int, int, int],\n",
    "    num_actions: int,\n",
    "    total_timesteps: int,\n",
    "    evaluation_rate: float,\n",
    ") -> float:\n",
    "    \"\"\"Optimization objective.\"\"\"\n",
    "    # Sample hyperparameters\n",
    "    hyperparams = sample_hyperparams(trial)\n",
    "\n",
    "    # Get configurations\n",
    "    dqn_config, training_config = get_configs_from_hyperparams(\n",
    "        total_timesteps=total_timesteps,\n",
    "        evaluation_rate=evaluation_rate,\n",
    "        observation_shape=observation_shape,\n",
    "        num_actions=num_actions,\n",
    "        hyperparams=hyperparams,\n",
    "    )\n",
    "\n",
    "    # Train DQN model\n",
    "    dqn, eval_stats = train_dqn(\n",
    "        env_config=env_config,\n",
    "        dqn_config=dqn_config,\n",
    "        training_config=training_config,\n",
    "        eval_env_config=eval_env_config,\n",
    "        trial=trial,\n",
    "    )\n",
    "\n",
    "    # Report evaluation results\n",
    "    return eval_stats[\"mean_score\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMESTEPS = 100_000\n",
    "EVAL_RATE = 0.2\n",
    "WARMUP_STEPS = int(NUM_TIMESTEPS * 2 * EVAL_RATE)\n",
    "NUM_TRIALS = 30\n",
    "NUM_STARTUP_TRIALS = 3\n",
    "\n",
    "# Select sampler\n",
    "sampler = optuna.samplers.GPSampler(n_startup_trials=NUM_STARTUP_TRIALS)\n",
    "\n",
    "# Select pruner\n",
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=NUM_STARTUP_TRIALS, n_warmup_steps=WARMUP_STEPS\n",
    ")\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "study.optimize(\n",
    "    func=partial(\n",
    "        objective,\n",
    "        env_config=env_config,\n",
    "        eval_env_config=eval_env_config,\n",
    "        observation_shape=(4, 84, 84),\n",
    "        num_actions=6,\n",
    "        total_timesteps=NUM_TIMESTEPS,\n",
    "        evaluation_rate=EVAL_RATE,\n",
    "    ),\n",
    "    n_trials=NUM_TRIALS,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "# Save study results\n",
    "study_results = study.trials_dataframe()\n",
    "study_results.to_csv(\"../models/deep-q-space-invaders/study_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [42:43<00:00, 117.03it/s]  \n"
     ]
    }
   ],
   "source": [
    "dqn_config, training_config = get_configs_from_hyperparams(\n",
    "    total_timesteps=300_000,\n",
    "    evaluation_rate=0.1,\n",
    "    observation_shape=(4, 84, 84),\n",
    "    num_actions=6,\n",
    "    hyperparams={\n",
    "        \"train_params\": {\n",
    "            \"exploration_fraction\": 0.10,\n",
    "            \"gamma\": 0.8,\n",
    "            \"target_inertia\": 0.995,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "model, results = train_dqn(\n",
    "    env_config=env_config,\n",
    "    dqn_config=dqn_config,\n",
    "    training_config=training_config,\n",
    "    eval_env_config=eval_env_config,\n",
    "    device=get_device(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign and save the model to local directory\n",
    "env_config = dotenv_values()\n",
    "\n",
    "sign_n_save_model(\n",
    "    model=model.cpu(),\n",
    "    destination=\"../models/deep-q-space-invaders\",\n",
    "    keypair=load_keypair(\"../models/private.pem\", env_config[\"MODEL_KEY_PASSWORD\"]),\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "agent: DQN = load_n_verify_model(\"../models/deep-q-space-invaders\")\n",
    "agent.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "num_episodes = 30\n",
    "eval_results = evaluate(\n",
    "    agent,\n",
    "    eval_env_config,\n",
    ")\n",
    "\n",
    "eval_stats = {\n",
    "    \"env_id\": eval_env_config.env_id,\n",
    "    \"num_episodes\": num_episodes,\n",
    "    \"mean_reward\": eval_results.mean_score,\n",
    "    \"std_reward\": eval_results.score_std,\n",
    "}\n",
    "\n",
    "with open(\"../models/deep-q-space-invaders/eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_stats, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = capture_replay(\n",
    "    env=eval_env_config.make_env(num_envs=1, seed=0).envs[0],\n",
    "    action_func=partial(agent.get_action, epsilon=0.0),\n",
    "    video_folder=\"../models/deep-q-space-invaders\",\n",
    "    max_steps=None,\n",
    ")\n",
    "print(f\"Total steps: {len(rewards)}\\nTotal reward: {sum(rewards)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2seedo_default_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
